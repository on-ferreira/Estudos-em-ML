{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a754b5f-8dd1-45cb-afad-41570e800599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65f6282-316b-4b70-a967-b00c88123d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor() #Definindo a conversão da imagem para tensor\n",
    "\n",
    "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #Carrega as imagens de treino do dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
    "\n",
    "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) #Carrega as imagens de validação do dataset [As imagens de teste]\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a5b41f5-cb39-49b6-90e8-c010d331f417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "imagens, etiquetas = next(dataiter)\n",
    "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47facd3-5788-4031-adcf-ec4d67831fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "print(imagens[0].shape) #Para verificar as dimensões do tensor de cada imagem\n",
    "print(etiquetas[0].shape) #Para verificar as dimensões do tensor de cada etiqueta\n",
    "print(etiquetas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b326bb7a-e14b-4ce5-b6dc-cc14c936e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 128) #Camada de entrada com 784 neurônios que se ligam a 128\n",
    "        self.linear2 = nn.Linear(128,64) #Camada interna 1, 128 neurônios que se ligam a 64\n",
    "        self.linear3 = nn.Linear(64,10) #Camada interna 2, 64 neurônios que se ligam a 10\n",
    "        #Para a camada de saída não é necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X)) #Função de ativação da camada de entrada para a camada interna 1\n",
    "        X = F.relu(self.linear2(X)) #Função de ativação da camada interna 1 para a camada interna 2\n",
    "        X = self.linear3(X) #Função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
    "        return F.log_softmax(X,dim=1) #Dados utilizados para a calcular a perda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd1d6575-5c8e-47d2-afbc-6870081284d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo, trainloader, device):\n",
    "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)\n",
    "    inicio = time()\n",
    "\n",
    "    criterio = nn.NLLLoss()\n",
    "    EPOCHS = 25\n",
    "    modelo.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0\n",
    "\n",
    "        for imagens, etiquetas in trainloader:\n",
    "            imagens = imagens.view(imagens.shape[0], -1)\n",
    "            otimizador.zero_grad()\n",
    "\n",
    "            output = modelo(imagens.to(device))\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device))\n",
    "\n",
    "            perda_instantanea.backward()\n",
    "            otimizador.step()\n",
    "\n",
    "            perda_acumulada += perda_instantanea.item()\n",
    "\n",
    "        print(\"Epoch {} - Perda resultante: {}\".format(epoch + 1, perda_acumulada / len(trainloader)))\n",
    "\n",
    "    print(\"\\n Tempo de treino (em minutos) = \", (time() - inicio) / 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef4a851d-3198-4746-a795-08df4a7b99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo, valloader, device):\n",
    "    conta_corretas, conta_todas = 0, 0\n",
    "    for imagens, etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1, 784) #Desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device)) #output do modelo em escala logaritimica\n",
    "    \n",
    "        ps = torch.exp(logps) #Converte o output para escala normal (Lembrando que é um tensor)\n",
    "        probab = list(ps.cpu().numpy()[0])\n",
    "        etiqueta_pred = probab.index(max(probab)) #Converte o tensor em um número, no caso o número que o modelo previu\n",
    "        etiqueta_certa = etiquetas.numpy()[i]\n",
    "        if(etiqueta_certa == etiqueta_pred): #Compara as etiquetas prevista e a correta\n",
    "            conta_corretas += 1\n",
    "        conta_todas +=1\n",
    "    \n",
    "    print(\"Total de imagens testadas =\", conta_todas)\n",
    "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5565c0bf-a13c-4c4f-b75b-7f4e4be32cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = Modelo()\n",
    "device = torch.device(\"cpu\")\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a527e77-e0a3-42e3-9b82-7f16fca014d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Perda resultante: 0.04243341888755814\n",
      "Epoch 2 - Perda resultante: 0.04048879063034903\n",
      "Epoch 3 - Perda resultante: 0.03862016515057685\n",
      "Epoch 4 - Perda resultante: 0.03702334354796421\n",
      "Epoch 5 - Perda resultante: 0.035282289089539697\n",
      "Epoch 6 - Perda resultante: 0.033744351522846106\n",
      "Epoch 7 - Perda resultante: 0.03249322252694581\n",
      "Epoch 8 - Perda resultante: 0.030777616435408727\n",
      "Epoch 9 - Perda resultante: 0.029663084126732894\n",
      "Epoch 10 - Perda resultante: 0.028302000290570235\n",
      "Epoch 11 - Perda resultante: 0.026976060767251966\n",
      "Epoch 12 - Perda resultante: 0.02585939287807721\n",
      "Epoch 13 - Perda resultante: 0.02480351950884906\n",
      "Epoch 14 - Perda resultante: 0.023827645513636152\n",
      "Epoch 15 - Perda resultante: 0.022654609933276094\n",
      "Epoch 16 - Perda resultante: 0.021609598718983714\n",
      "Epoch 17 - Perda resultante: 0.020682190005892893\n",
      "Epoch 18 - Perda resultante: 0.019835484714415083\n",
      "Epoch 19 - Perda resultante: 0.019038931403839304\n",
      "Epoch 20 - Perda resultante: 0.018149599508280115\n",
      "Epoch 21 - Perda resultante: 0.017435475604421\n",
      "Epoch 22 - Perda resultante: 0.016764186542457107\n",
      "Epoch 23 - Perda resultante: 0.016054315543023626\n",
      "Epoch 24 - Perda resultante: 0.015338961490993895\n",
      "Epoch 25 - Perda resultante: 0.01485978289339036\n",
      "\n",
      " Tempo de treino (em minutos) =  2.626566195487976\n"
     ]
    }
   ],
   "source": [
    "treino(modelo,trainloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bca20884-752e-4535-a36b-aad7a712332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imagens testadas = 157\n",
      "\n",
      "Precisão do modelo = 96.81528662420382%\n"
     ]
    }
   ],
   "source": [
    "validacao(modelo, valloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f8f8999-250a-4d1d-bfa0-bf63911095e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades para a imagem 0.png:\n",
      "Classe 0: 0.5694\n",
      "Classe 1: 0.0000\n",
      "Classe 2: 0.0003\n",
      "Classe 3: 0.0000\n",
      "Classe 4: 0.0000\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 0.0003\n",
      "Classe 8: 0.0000\n",
      "Classe 9: 0.4299\n",
      "Probabilidades para a imagem 1.png:\n",
      "Classe 0: 0.0000\n",
      "Classe 1: 0.0006\n",
      "Classe 2: 0.0009\n",
      "Classe 3: 0.0091\n",
      "Classe 4: 0.0000\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 0.6721\n",
      "Classe 8: 0.0000\n",
      "Classe 9: 0.3173\n",
      "Probabilidades para a imagem 3.png:\n",
      "Classe 0: 0.0000\n",
      "Classe 1: 0.0000\n",
      "Classe 2: 0.0000\n",
      "Classe 3: 1.0000\n",
      "Classe 4: 0.0000\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 0.0000\n",
      "Classe 8: 0.0000\n",
      "Classe 9: 0.0000\n",
      "Probabilidades para a imagem 4.png:\n",
      "Classe 0: 0.0000\n",
      "Classe 1: 0.0000\n",
      "Classe 2: 0.0000\n",
      "Classe 3: 0.0001\n",
      "Classe 4: 0.1704\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 0.0010\n",
      "Classe 8: 0.0001\n",
      "Classe 9: 0.8284\n",
      "Probabilidades para a imagem 7.png:\n",
      "Classe 0: 0.0000\n",
      "Classe 1: 0.0000\n",
      "Classe 2: 0.0000\n",
      "Classe 3: 0.0000\n",
      "Classe 4: 0.0000\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 1.0000\n",
      "Classe 8: 0.0000\n",
      "Classe 9: 0.0000\n",
      "Probabilidades para a imagem 8.png:\n",
      "Classe 0: 0.0000\n",
      "Classe 1: 0.0000\n",
      "Classe 2: 0.0000\n",
      "Classe 3: 0.0000\n",
      "Classe 4: 0.0000\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 0.0000\n",
      "Classe 8: 1.0000\n",
      "Classe 9: 0.0000\n",
      "Probabilidades para a imagem 9.png:\n",
      "Classe 0: 0.0000\n",
      "Classe 1: 0.0000\n",
      "Classe 2: 0.0000\n",
      "Classe 3: 0.0000\n",
      "Classe 4: 0.0000\n",
      "Classe 5: 0.0000\n",
      "Classe 6: 0.0000\n",
      "Classe 7: 0.0000\n",
      "Classe 8: 0.0164\n",
      "Classe 9: 0.9836\n",
      "Imagem 0.png: Classe predita = 0\n",
      "Imagem 1.png: Classe predita = 7\n",
      "Imagem 3.png: Classe predita = 3\n",
      "Imagem 4.png: Classe predita = 9\n",
      "Imagem 7.png: Classe predita = 7\n",
      "Imagem 8.png: Classe predita = 8\n",
      "Imagem 9.png: Classe predita = 9\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Caminho para a pasta contendo as imagens\n",
    "pasta_imagens = 'C:/Users/orlan/OneDrive/Documentos/UFF/DIO/ML/imagens/'\n",
    "\n",
    "# Lista para armazenar os resultados das previsões\n",
    "resultados = []\n",
    "\n",
    "# Percorrer todas as imagens na pasta\n",
    "for filename in os.listdir(pasta_imagens):\n",
    "    # Verificar se é um arquivo de imagem\n",
    "    if filename.endswith(\".png\"):\n",
    "        # Caminho completo para a imagem\n",
    "        caminho_imagem = os.path.join(pasta_imagens, filename)\n",
    "\n",
    "        # Carregar a imagem usando a biblioteca PIL\n",
    "        imagem = Image.open(caminho_imagem)\n",
    "\n",
    "        # Redimensionar a imagem para 28x28 pixels\n",
    "        imagem = imagem.resize((28, 28))\n",
    "\n",
    "        # Converter a imagem para escala de cinza\n",
    "        imagem = imagem.convert('L')\n",
    "\n",
    "        # Converter a imagem para um array NumPy e normalizar os valores para o intervalo [0, 1]\n",
    "        imagem_array = np.array(imagem) / 255.0\n",
    "\n",
    "        # Redimensionar o array para corresponder à entrada do modelo (que espera um batch de tamanho 1)\n",
    "        imagem_array = imagem_array.reshape(1, -1)\n",
    "\n",
    "        # Converter o array para o formato Tensor do PyTorch\n",
    "        imagem_tensor = torch.tensor(imagem_array, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = modelo(imagem_tensor)\n",
    "\n",
    "        # Obter as probabilidades para cada classe\n",
    "        probabilidades = torch.exp(output)\n",
    "\n",
    "        # Obter o índice da classe com a maior probabilidade\n",
    "        classe_predita = torch.argmax(probabilidades)\n",
    "\n",
    "        # Armazenar o resultado da previsão (índice da classe predita) na lista de resultados\n",
    "        resultados.append(classe_predita.item())\n",
    "\n",
    "        # Obter as probabilidades para cada classe\n",
    "        probabilidades = torch.exp(output)\n",
    "\n",
    "        # Imprimir as probabilidades para cada classe\n",
    "        print(f\"Probabilidades para a imagem {filename}:\")\n",
    "        for classe, probabilidade in enumerate(probabilidades.squeeze().tolist()):\n",
    "            print(f\"Classe {classe}: {probabilidade:.4f}\")\n",
    "\n",
    "\n",
    "# Imprimir os resultados\n",
    "for i, filename in enumerate(os.listdir(pasta_imagens)):\n",
    "    if filename.endswith(\".png\"):\n",
    "        print(f\"Imagem {filename}: Classe predita = {resultados[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
